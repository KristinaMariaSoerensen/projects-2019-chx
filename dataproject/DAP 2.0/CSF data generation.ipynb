{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Classical Swine Fever outbreak dataset - Japan 2018/2019\n",
    "\n",
    "This workbook generates a .csv-file with data on the current outbreaks of classical swine fever in Japan. \n",
    "\n",
    "The data is scraped from the following site: http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?reportid=27871\n",
    "\n",
    "Every week, give or take, a new report is published. The entire data collection process is rather slow - maybe 10-20 min. in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One url such as: http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=29772 only contains the data of one report. To collect all data from all reports I have to scrape all existing urls for all existing reports. Luckily all reports contain links to all other reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To collect all urls I start by scraping the html from one of the reports. Which report should be unimportant.\n",
    "url_random = 'http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=29772'\n",
    "response = requests.get(url_random)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperlinks in the initial webpage: 29\n",
      "[None, \"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27871')\"]\n",
      "Number of relevant hyperlinks: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27871')\",\n",
       " \"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27924')\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To collect all hyperlinks from the html BeautifulSoup finds all html codes with 'a' and 'href'.  \n",
    "links = []\n",
    "for url in soup.find_all('a'):\n",
    "    link = url.get('href')\n",
    "    links.append(link)\n",
    "\n",
    "print('Number of hyperlinks in the initial webpage:', len(links))\n",
    "print(links[:2])\n",
    "\n",
    "## The first link collected is not for a report, so it is excluded:\n",
    "links = links[1:]\n",
    "print('Number of relevant hyperlinks:', len(links))\n",
    "links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of report numbers: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['27871', '27924']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From each hyperlink i collect the unique report-ID. OBS: The following code is sensitive to changes.\n",
    "report_number = []\n",
    "\n",
    "for link in links:                            \n",
    "    link = link[-7:-2]                        # First time I tried this code, the index was from -6:-1\n",
    "    if link[0] == '2':                                            \n",
    "        report_number.append(link)                               \n",
    "    if link[0] == '3':                        # First time I tried this all report codes started with '2'\n",
    "        report_number.append(link)\n",
    "    \n",
    "print('Number of report numbers:', len(report_number))\n",
    "report_number[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=27871',\n",
       " 'http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=27924']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I combine the reportid collected in the report_number list with the standard URL for all the hyperlinks\n",
    "all_links = ['http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid='+ i for i in report_number]\n",
    "print('Number of links:', len(all_links))\n",
    "\n",
    "all_links[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of html scrapes: 28\n"
     ]
    }
   ],
   "source": [
    "## Here I loop through all the URLs to make a list of the html of all the reports.\n",
    "## Takes around 10 minutes to load due to slow response from oie.int and a time delay. \n",
    "##(The delay might be adjusted without problems though)\n",
    "\n",
    "all_html = []\n",
    "for link in all_links:\n",
    "    sleep(randint(8,15))\n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html')\n",
    "    all_html.append(soup)\n",
    "    \n",
    "print('Number of html scrapes:', len(all_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now I sort and alter the data into lists to get my data columns.\n",
    "\n",
    "## Empty lists to make our data columns for each relevant scrape\n",
    "outbreak_id = []\n",
    "city = []\n",
    "prefecture = []\n",
    "outbreak_date = []\n",
    "species = []\n",
    "no_susceptible = []\n",
    "no_cases = []\n",
    "no_deaths = []\n",
    "\n",
    "## Looping through all the report html scrapes\n",
    "for report in all_html:\n",
    "    ## I find the report number\n",
    "    report_no = report.find('td', {'width':'30%'}).text[-2:]\n",
    "    \n",
    "    ## Empty list of outbreaks\n",
    "    outbreaks = []\n",
    "    \n",
    "    ## The tables are collected from the html code\n",
    "    tables = report.find_all('table', {'class':'TableFoyers'})\n",
    "    for tr in tables:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text.strip() for tr in td]\n",
    "        for i in row:\n",
    "            text = str(i)\n",
    "            if 'Outbreak' in text and any(char.isdigit() for char in text):\n",
    "                outbreaks.append(row) \n",
    "\n",
    "    #\n",
    "    outbreaks = [[element or '0' for element in outbreak] for outbreak in outbreaks]\n",
    "    for outbreak in outbreaks:\n",
    "        \n",
    "        ## outbreak_id (the report number added the outbreak counter):\n",
    "        out = outbreak[0]\n",
    "        out2 = report_no + '.' + out[9:100]\n",
    "        outbreak_id.append(out2)\n",
    "        \n",
    "        ## The city and prefecture\n",
    "        out = outbreak[1]\n",
    "        out = out.replace('-',' ')\n",
    "        split = out.split(\" \")\n",
    "        city.append(split[0])\n",
    "        if split[2] == 'City,':                                     # Must be added because one is called \"Higashi Osaka City, Osaka\"\n",
    "            prefecture.append(split[3])\n",
    "        else:\n",
    "            prefecture.append(split[2])\n",
    "        \n",
    "        ## Date of outbreak\n",
    "        out = datetime.strptime(outbreak[3], '%d/%m/%Y').date()\n",
    "        outbreak_date.append(out)\n",
    "    \n",
    "        ## Species\n",
    "        out = outbreak[16]\n",
    "        species.append(out)\n",
    "    \n",
    "        ## Number of susceptible animals\n",
    "        out = outbreak[17]\n",
    "        no_susceptible.append(out)\n",
    "    \n",
    "        ## Number of cases\n",
    "        out = outbreak[18]\n",
    "        no_cases.append(out)\n",
    "        \n",
    "        ## Number of deaths\n",
    "        out = outbreak[19]\n",
    "        no_deaths.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outbreaks: 291\n"
     ]
    }
   ],
   "source": [
    "# To check format and observations:\n",
    "print('Number of outbreaks:', len(city))\n",
    "\n",
    "#print(outbreak_id)\n",
    "#print(city)\n",
    "#print(prefecture)\n",
    "#print(outbreak_date)\n",
    "#print(len(species))\n",
    "#print(no_susceptible)\n",
    "#print(no_cases)\n",
    "#print(no_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnally I make the dataset of all the collected data from the OIE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291 entries, 0 to 290\n",
      "Data columns (total 8 columns):\n",
      "ID              291 non-null object\n",
      "City            291 non-null object\n",
      "Prefecture      291 non-null object\n",
      "Date            291 non-null object\n",
      "Species         291 non-null object\n",
      "Susceptibles    291 non-null object\n",
      "Cases           291 non-null object\n",
      "Deaths          291 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 18.3+ KB\n",
      "None\n",
      "          ID  City Prefecture        Date                       Species  \\\n",
      "0  on.1  (1)  Gifu       Gifu  2018-09-03                         Swine   \n",
      "1        1.1  Gifu       Gifu  2018-09-13  Wild boar:Sus scrofa(Suidae)   \n",
      "2        2.1  Gifu       Gifu  2018-09-15  Wild boar:Sus scrofa(Suidae)   \n",
      "3        2.2  Gifu       Gifu  2018-09-18  Wild boar:Sus scrofa(Suidae)   \n",
      "4        2.3  Gifu       Gifu  2018-09-21  Wild boar:Sus scrofa(Suidae)   \n",
      "\n",
      "  Susceptibles Cases Deaths  \n",
      "0          610    29     29  \n",
      "1            0     1      1  \n",
      "2            0     1      1  \n",
      "3            0     3      3  \n",
      "4            0     1      1  \n"
     ]
    }
   ],
   "source": [
    "## The lists are converted into a combined dataframe:\n",
    "csf = pd.DataFrame({'ID': outbreak_id,\n",
    "                              'City': city,\n",
    "                              'Prefecture': prefecture,\n",
    "                              'Date': outbreak_date,\n",
    "                              'Species': species,\n",
    "                              'Susceptibles': no_susceptible,\n",
    "                              'Cases': no_cases,\n",
    "                              'Deaths': no_deaths                            \n",
    "})\n",
    "\n",
    "print(csf.info())\n",
    "print(csf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of outbreak coordinates\n",
    "I want to be able to make a map so I also collect geospatial data. For this I use GeoLocator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the city names are corrected, the city names can be used to collect the coordinates from GeoLocator:\n",
    "new = csf['City'].str.split()\n",
    "csf['City'] = new.str[0]\n",
    "\n",
    "csf = csf.replace(to_replace ='Ohmihachiman', value ='Omihachiman')  ## Ohmihachiman does not work with GeoLocator\n",
    "\n",
    "csf['Prefecture'] = np.where(csf['City'] == 'Tahara', 'Aichi', csf['Prefecture']) ## Report 26 states two outbreaks in Tahara, Gifu, but Tahara is in Aichi\n",
    "\n",
    "csf['Geo_Lookup'] = csf['City'] + ' ' + csf['Prefecture']\n",
    "csf = csf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBS: Very slow (6 min.) - to avoid being cut off I include a timeout ###\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"school project\")\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "for city in csf['Geo_Lookup']:\n",
    "    try:\n",
    "        location = geolocator.geocode(city, timeout=50)\n",
    "        lat = location.latitude\n",
    "        lon = location.longitude\n",
    "        latitude.append(lat)\n",
    "        longitude.append(lon)\n",
    "    except:\n",
    "        print(city)      # If an error occurs the whole loop stops. This is included to know what throws the error.\n",
    "        raise\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "# I check that the length matches the number of observations:\n",
    "print(len(latitude))\n",
    "print(len(longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291 entries, 0 to 290\n",
      "Data columns (total 2 columns):\n",
      "Latitude     291 non-null float64\n",
      "Longitude    291 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 4.6 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291 entries, 0 to 290\n",
      "Data columns (total 9 columns):\n",
      "ID              291 non-null object\n",
      "City            291 non-null object\n",
      "Prefecture      291 non-null object\n",
      "Date            291 non-null object\n",
      "Species         291 non-null object\n",
      "Susceptibles    291 non-null object\n",
      "Cases           291 non-null object\n",
      "Deaths          291 non-null object\n",
      "Geo_Lookup      291 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 20.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The list with the coordinates are converted to a dataframe\n",
    "coordinates = pd.DataFrame({'Latitude': latitude,\n",
    "                              'Longitude': longitude,                            \n",
    "})\n",
    "\n",
    "# I check the compatability\n",
    "print(coordinates.info())\n",
    "print(csf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID          City Prefecture        Date  \\\n",
      "0        on.1  (1)          Gifu       Gifu  2018-09-03   \n",
      "1              1.1          Gifu       Gifu  2018-09-13   \n",
      "2              2.1          Gifu       Gifu  2018-09-15   \n",
      "3              2.2          Gifu       Gifu  2018-09-18   \n",
      "4              2.3          Gifu       Gifu  2018-09-21   \n",
      "5              3.1          Gifu       Gifu  2018-09-26   \n",
      "6              3.2  Kakamigahara       Gifu  2018-09-27   \n",
      "7              3.3  Kakamigahara       Gifu  2018-09-28   \n",
      "8              3.4          Gifu       Gifu  2018-09-28   \n",
      "9              4.1          Gifu       Gifu  2018-09-30   \n",
      "10             4.2          Gifu       Gifu  2018-10-02   \n",
      "11             4.3  Kakamigahara       Gifu  2018-10-03   \n",
      "12             4.4  Kakamigahara       Gifu  2018-10-03   \n",
      "13             4.5          Gifu       Gifu  2018-10-04   \n",
      "14             4.6          Gifu       Gifu  2018-10-05   \n",
      "15             4.7          Gifu       Gifu  2018-10-05   \n",
      "16             4.8          Gifu       Gifu  2018-10-07   \n",
      "17             4.9  Kakamigahara       Gifu  2018-10-08   \n",
      "18        5.1  (2)          Gifu       Gifu  2018-11-15   \n",
      "19     7.cluster 1          Gifu       Gifu  2018-10-09   \n",
      "20     7.cluster 2  Kakamigahara       Gifu  2018-10-12   \n",
      "21     7.cluster 3          Seki       Gifu  2018-10-19   \n",
      "22             7.4          Kani       Gifu  2018-10-30   \n",
      "23             7.5      Yamagata       Gifu  2018-11-01   \n",
      "24             7.6      Sakahogi       Gifu  2018-11-24   \n",
      "25             7.7        Yaotsu       Gifu  2018-11-26   \n",
      "26        7.8  (3)      Minokamo       Gifu  2018-12-03   \n",
      "27             8.1  Kakamigahara       Gifu  2018-11-30   \n",
      "28             8.2        Yaotsu       Gifu  2018-12-02   \n",
      "29             8.3          Kani       Gifu  2018-12-04   \n",
      "..             ...           ...        ...         ...   \n",
      "257     26.9  (21)        Tahara      Aichi  2019-04-20   \n",
      "258    26.10  (21)        Tahara      Aichi  2019-04-20   \n",
      "259    26.11  (21)        Tahara      Aichi  2019-04-20   \n",
      "260  26.cluster 12          Gujo       Gifu  2019-04-20   \n",
      "261          26.13        Motosu       Gifu  2019-04-20   \n",
      "262  26.cluster 14      Yamagata       Gifu  2019-04-20   \n",
      "263  26.cluster 15        Mitake       Gifu  2019-04-20   \n",
      "264    26.16  (22)          Seto      Aichi  2019-04-21   \n",
      "265  26.cluster 17     Shirakawa       Gifu  2019-04-22   \n",
      "266  26.cluster 18       Hichiso       Gifu  2019-04-22   \n",
      "267          26.19   Nakatsugawa       Gifu  2019-04-22   \n",
      "268          26.20        Tomika       Gifu  2019-04-22   \n",
      "269          26.21        Kawabe       Gifu  2019-04-22   \n",
      "270      27.1  (7)          Gifu       Gifu  2019-01-29   \n",
      "271      27.2  (8)     Matsumoto     Nagano  2019-02-06   \n",
      "272           27.3       Inuyama      Aichi  2019-04-12   \n",
      "273           27.4       Inuyama      Aichi  2019-04-20   \n",
      "274           27.5          Toki       Gifu  2019-04-25   \n",
      "275           27.6          Gujo       Gifu  2019-04-25   \n",
      "276           27.7      Minokamo       Gifu  2019-04-25   \n",
      "277   27.cluster 8        Mitake       Gifu  2019-04-26   \n",
      "278           27.9        Kawabe       Gifu  2019-04-26   \n",
      "279  27.cluster 10           Ena       Gifu  2019-04-26   \n",
      "280          27.11      Mizunami       Gifu  2019-04-26   \n",
      "281          27.12          Mino       Gifu  2019-04-26   \n",
      "282          27.13        Motosu       Gifu  2019-04-28   \n",
      "283  27.cluster 14        Yaotsu       Gifu  2019-04-29   \n",
      "284          27.15          Kani       Gifu  2019-04-30   \n",
      "285  27.cluster 16        Tajimi       Gifu  2019-05-01   \n",
      "286  27.cluster 17     Shirakawa       Gifu  2019-05-06   \n",
      "\n",
      "                          Species Susceptibles Cases Deaths  \\\n",
      "0                           Swine          610    29     29   \n",
      "1    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "2    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "3    Wild boar:Sus scrofa(Suidae)            0     3      3   \n",
      "4    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "5    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "6    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "7    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "8    Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "9    Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "10   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "11   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "12   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "13   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "14   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "15   Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "16   Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "17   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "18                          Swine           23     2      0   \n",
      "19   Wild boar:Sus scrofa(Suidae)            0    18      8   \n",
      "20   Wild boar:Sus scrofa(Suidae)            0    12      2   \n",
      "21   Wild boar:Sus scrofa(Suidae)            0     7      3   \n",
      "22   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "23   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "24   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "25   Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "26                          Swine          491     4      0   \n",
      "27   Wild boar:Sus scrofa(Suidae)            0     2      1   \n",
      "28   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "29   Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "..                            ...          ...   ...    ...   \n",
      "257                         Swine         1024     0      0   \n",
      "258                         Swine          311     0      0   \n",
      "259                         Swine          391     0      0   \n",
      "260  Wild boar:Sus scrofa(Suidae)            0     2      0   \n",
      "261  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "262  Wild boar:Sus scrofa(Suidae)            0     2      0   \n",
      "263  Wild boar:Sus scrofa(Suidae)            0     4      1   \n",
      "264                         Swine          966     0      0   \n",
      "265  Wild boar:Sus scrofa(Suidae)            0     2      1   \n",
      "266  Wild boar:Sus scrofa(Suidae)            0     2      0   \n",
      "267  Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "268  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "269  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "270                         Swine          149     0      0   \n",
      "271                         Swine           38     0      0   \n",
      "272  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "273  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "274  Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "275  Wild boar:Sus scrofa(Suidae)            0     7      4   \n",
      "276  Wild boar:Sus scrofa(Suidae)            0     5      4   \n",
      "277  Wild boar:Sus scrofa(Suidae)            0     3      3   \n",
      "278  Wild boar:Sus scrofa(Suidae)            0     1      0   \n",
      "279  Wild boar:Sus scrofa(Suidae)            0     9      9   \n",
      "280  Wild boar:Sus scrofa(Suidae)            0     3      3   \n",
      "281  Wild boar:Sus scrofa(Suidae)            0     1      1   \n",
      "282  Wild boar:Sus scrofa(Suidae)            0     3      1   \n",
      "283  Wild boar:Sus scrofa(Suidae)            0     2      2   \n",
      "284  Wild boar:Sus scrofa(Suidae)            0     4      3   \n",
      "285  Wild boar:Sus scrofa(Suidae)            0     5      3   \n",
      "286  Wild boar:Sus scrofa(Suidae)            0     4      3   \n",
      "\n",
      "            Geo_Lookup   Latitude   Longitude  \n",
      "0            Gifu Gifu  35.423095  136.762753  \n",
      "1            Gifu Gifu  35.423095  136.762753  \n",
      "2            Gifu Gifu  35.423095  136.762753  \n",
      "3            Gifu Gifu  35.423095  136.762753  \n",
      "4            Gifu Gifu  35.423095  136.762753  \n",
      "5            Gifu Gifu  35.423095  136.762753  \n",
      "6    Kakamigahara Gifu  35.399583  136.848565  \n",
      "7    Kakamigahara Gifu  35.399583  136.848565  \n",
      "8            Gifu Gifu  35.423095  136.762753  \n",
      "9            Gifu Gifu  35.423095  136.762753  \n",
      "10           Gifu Gifu  35.423095  136.762753  \n",
      "11   Kakamigahara Gifu  35.399583  136.848565  \n",
      "12   Kakamigahara Gifu  35.399583  136.848565  \n",
      "13           Gifu Gifu  35.423095  136.762753  \n",
      "14           Gifu Gifu  35.423095  136.762753  \n",
      "15           Gifu Gifu  35.423095  136.762753  \n",
      "16           Gifu Gifu  35.423095  136.762753  \n",
      "17   Kakamigahara Gifu  35.399583  136.848565  \n",
      "18           Gifu Gifu  35.423095  136.762753  \n",
      "19           Gifu Gifu  35.423095  136.762753  \n",
      "20   Kakamigahara Gifu  35.399583  136.848565  \n",
      "21           Seki Gifu  35.495803  136.918148  \n",
      "22           Kani Gifu  35.426109  137.061317  \n",
      "23       Yamagata Gifu  35.409486  136.756977  \n",
      "24       Sakahogi Gifu  35.429904  136.978499  \n",
      "25         Yaotsu Gifu  35.475594  137.141068  \n",
      "26       Minokamo Gifu  35.440655  137.015505  \n",
      "27   Kakamigahara Gifu  35.399583  136.848565  \n",
      "28         Yaotsu Gifu  35.475594  137.141068  \n",
      "29           Kani Gifu  35.426109  137.061317  \n",
      "..                 ...        ...         ...  \n",
      "257       Tahara Aichi  34.639112  137.183207  \n",
      "258       Tahara Aichi  34.639112  137.183207  \n",
      "259       Tahara Aichi  34.639112  137.183207  \n",
      "260          Gujo Gifu  35.748417  136.964310  \n",
      "261        Motosu Gifu  35.423095  136.762753  \n",
      "262      Yamagata Gifu  35.409486  136.756977  \n",
      "263        Mitake Gifu  35.431613  137.129584  \n",
      "264         Seto Aichi  35.240984  137.116187  \n",
      "265     Shirakawa Gifu  35.580840  137.189547  \n",
      "266       Hichiso Gifu  35.542578  137.119852  \n",
      "267   Nakatsugawa Gifu  35.487646  137.500540  \n",
      "268        Tomika Gifu  35.485146  136.976204  \n",
      "269        Kawabe Gifu  35.486513  137.068158  \n",
      "270          Gifu Gifu  35.423095  136.762753  \n",
      "271   Matsumoto Nagano  36.238205  137.968714  \n",
      "272      Inuyama Aichi  35.360969  136.984018  \n",
      "273      Inuyama Aichi  35.360969  136.984018  \n",
      "274          Toki Gifu  35.352485  137.183419  \n",
      "275          Gujo Gifu  35.748417  136.964310  \n",
      "276      Minokamo Gifu  35.440655  137.015505  \n",
      "277        Mitake Gifu  35.431613  137.129584  \n",
      "278        Kawabe Gifu  35.486513  137.068158  \n",
      "279           Ena Gifu  35.449809  137.412827  \n",
      "280      Mizunami Gifu  35.361993  137.254167  \n",
      "281          Mino Gifu  35.544262  136.907518  \n",
      "282        Motosu Gifu  35.423095  136.762753  \n",
      "283        Yaotsu Gifu  35.475594  137.141068  \n",
      "284          Kani Gifu  35.426109  137.061317  \n",
      "285        Tajimi Gifu  35.335757  137.127762  \n",
      "286     Shirakawa Gifu  35.580840  137.189547  \n",
      "\n",
      "[287 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# The coordinates-dataframe is appended to the big dataset \n",
    "csf['Latitude'] = coordinates['Latitude']\n",
    "csf['Longitude'] = coordinates['Longitude']\n",
    "\n",
    "csf = csf\n",
    "\n",
    "print(csf.head(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = csf.to_csv('CSF_Japan_data.csv', index = False)\n",
    "\n",
    "# I create a folder for all generated datasets, just for safe keeping.\n",
    "if not os.path.exists('Old datasets'):\n",
    "    os.mkdir('Old datasets')\n",
    "\n",
    "# I save an additional CSV with todays date to the old datasets folder.\n",
    "todays_date = datetime.now().strftime('%d%m%Y')\n",
    "export_csv_date = csf.to_csv('Old datasets/CSF_Outbreaks_Japan_'+todays_date+'.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
