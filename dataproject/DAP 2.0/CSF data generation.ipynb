{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Classical Swine Fever outbreak dataset - Japan 2018/2019\n",
    "\n",
    "This workbook generates a .csv-file with data on the current outbreaks of classical swine fever in Japan. \n",
    "\n",
    "The data is scraped from the following site: http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?reportid=27871\n",
    "\n",
    "Every week, give or take, a new report is published. The entire data collection process is rather slow - maybe 10-20 min. in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One url such as: http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=29772 only contains the data of one report. To collect all data from all reports I have to scrape all existing urls for all existing reports. Luckily all reports contain links to all other reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To collect all urls I start by scraping the html from one of the reports. Which report should be unimportant.\n",
    "url_random = 'http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=29772'\n",
    "response = requests.get(url_random)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperlinks in the initial webpage: 28\n",
      "[None, \"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27871')\"]\n",
      "Number of relevant hyperlinks: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27871')\",\n",
       " \"javascript:open_report('/wahis_2/public/wahid.php/Reviewreport/Review?', '27924')\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To collect all hyperlinks from the html BeautifulSoup finds all html codes with 'a' and 'href'.  \n",
    "links = []\n",
    "for url in soup.find_all('a'):\n",
    "    link = url.get('href')\n",
    "    links.append(link)\n",
    "\n",
    "print('Number of hyperlinks in the initial webpage:', len(links))\n",
    "print(links[:2])\n",
    "\n",
    "## The first link collected is not for a report, so it is excluded:\n",
    "links = links[1:]\n",
    "print('Number of relevant hyperlinks:', len(links))\n",
    "links[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of report numbers: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['27871', '27924']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From each hyperlink i collect the unique report-ID. OBS: The following code is sensitive to changes.\n",
    "report_number = []\n",
    "\n",
    "for link in links:                            \n",
    "    link = link[-7:-2]                        # First time I tried this code, the index was from -6:-1\n",
    "    if link[0] == '2':                                            \n",
    "        report_number.append(link)                               \n",
    "    if link[0] == '3':                        # First time I tried this all report codes started with '2'\n",
    "        report_number.append(link)\n",
    "    \n",
    "print('Number of report numbers:', len(report_number))\n",
    "report_number[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=27871',\n",
       " 'http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid=27924']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I combine the reportid collected in the report_number list with the standard URL for all the hyperlinks\n",
    "all_links = ['http://www.oie.int/wahis_2/public/wahid.php/Reviewreport/Review?page_refer=MapFullEventReport&reportid='+ i for i in report_number]\n",
    "print('Number of links:', len(all_links))\n",
    "\n",
    "all_links[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of html scrapes: 27\n"
     ]
    }
   ],
   "source": [
    "## Here I loop through all the URLs to make a list of the html of all the reports.\n",
    "## Takes around 10 minutes to load due to slow response from oie.int and a time delay. \n",
    "##(The delay might be adjusted without problems though)\n",
    "\n",
    "all_html = []\n",
    "for link in all_links:\n",
    "    sleep(randint(8,15))\n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html')\n",
    "    all_html.append(soup)\n",
    "    \n",
    "print('Number of html scrapes:', len(all_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now I sort and alter the data into lists to get my data columns.\n",
    "\n",
    "## Empty lists to make our data columns for each relevant scrape\n",
    "outbreak_id = []\n",
    "city = []\n",
    "prefecture = []\n",
    "outbreak_date = []\n",
    "species = []\n",
    "no_susceptible = []\n",
    "no_cases = []\n",
    "no_deaths = []\n",
    "\n",
    "## Looping through all the report html scrapes\n",
    "for report in all_html:\n",
    "    ## I find the report number\n",
    "    report_no = report.find('td', {'width':'30%'}).text[-2:]\n",
    "    \n",
    "    ## Empty list of outbreaks\n",
    "    outbreaks = []\n",
    "    \n",
    "    ## The tables are collected from the html code\n",
    "    tables = report.find_all('table', {'class':'TableFoyers'})\n",
    "    for tr in tables:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text.strip() for tr in td]\n",
    "        for i in row:\n",
    "            text = str(i)\n",
    "            for i in range(1,1000):                                 # Thousand is chosen as it is an unreasonably high number\n",
    "                if text == 'Outbreak '+ str(i):\n",
    "                    outbreaks.append(row)\n",
    "                if text == 'Outbreak cluster '+ str(i):\n",
    "                    outbreaks.append(row)    \n",
    "\n",
    "    #\n",
    "    outbreaks = [[element or '0' for element in outbreak] for outbreak in outbreaks]\n",
    "    for outbreak in outbreaks:\n",
    "        \n",
    "        ## outbreak_id (the report number added the outbreak counter):\n",
    "        out = outbreak[0]\n",
    "        out2 = report_no + '.' + out[9:100]\n",
    "        outbreak_id.append(out2)\n",
    "        \n",
    "        ## The city and prefecture\n",
    "        out = outbreak[1]\n",
    "        out = out.replace('-',' ')\n",
    "        split = out.split(\" \")\n",
    "        city.append(split[0])\n",
    "        if split[2] == 'City,':                                     # Must be added because one is called \"Higashi Osaka City, Osaka\"\n",
    "            prefecture.append(split[3])\n",
    "        else:\n",
    "            prefecture.append(split[2])\n",
    "        \n",
    "        ## Date of outbreak\n",
    "        out = datetime.strptime(outbreak[3], '%d/%m/%Y').date()\n",
    "        outbreak_date.append(out)\n",
    "    \n",
    "        ## Species\n",
    "        out = outbreak[16]\n",
    "        species.append(out)\n",
    "    \n",
    "        ## Number of susceptible animals\n",
    "        out = outbreak[17]\n",
    "        no_susceptible.append(out)\n",
    "    \n",
    "        ## Number of cases\n",
    "        out = outbreak[18]\n",
    "        no_cases.append(out)\n",
    "        \n",
    "        ## Number of deaths\n",
    "        out = outbreak[19]\n",
    "        no_deaths.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outbreaks: 270\n"
     ]
    }
   ],
   "source": [
    "# To check format and observations:\n",
    "print('Number of outbreaks:', len(outbreak_id))\n",
    "\n",
    "#print(outbreak_id)\n",
    "#print(city)\n",
    "#print(prefecture)\n",
    "#print(outbreak_date)\n",
    "#print(len(species))\n",
    "#print(no_susceptible)\n",
    "#print(no_cases)\n",
    "#print(no_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnally I make the dataset of all the collected data from the OIE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 270 entries, 0 to 269\n",
      "Data columns (total 8 columns):\n",
      "ID              270 non-null object\n",
      "City            270 non-null object\n",
      "Prefecture      270 non-null object\n",
      "Date            270 non-null object\n",
      "Species         270 non-null object\n",
      "Susceptibles    270 non-null object\n",
      "Cases           270 non-null object\n",
      "Deaths          270 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 17.0+ KB\n",
      "None\n",
      "     ID  City Prefecture        Date                       Species  \\\n",
      "0  on.1  Gifu       Gifu  2018-09-03                         Swine   \n",
      "1   1.1  Gifu       Gifu  2018-09-13  Wild boar:Sus scrofa(Suidae)   \n",
      "2   2.1  Gifu       Gifu  2018-09-15  Wild boar:Sus scrofa(Suidae)   \n",
      "3   2.2  Gifu       Gifu  2018-09-18  Wild boar:Sus scrofa(Suidae)   \n",
      "4   2.3  Gifu       Gifu  2018-09-21  Wild boar:Sus scrofa(Suidae)   \n",
      "\n",
      "  Susceptibles Cases Deaths  \n",
      "0          610    29     29  \n",
      "1            0     1      1  \n",
      "2            0     1      1  \n",
      "3            0     3      3  \n",
      "4            0     1      1  \n"
     ]
    }
   ],
   "source": [
    "## The lists are converted into a combined dataframe:\n",
    "csf = pd.DataFrame({'ID': outbreak_id,\n",
    "                              'City': city,\n",
    "                              'Prefecture': prefecture,\n",
    "                              'Date': outbreak_date,\n",
    "                              'Species': species,\n",
    "                              'Susceptibles': no_susceptible,\n",
    "                              'Cases': no_cases,\n",
    "                              'Deaths': no_deaths                            \n",
    "})\n",
    "\n",
    "print(csf.info())\n",
    "print(csf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection of outbreak coordinates\n",
    "I want to be able to make a map so I also collect geospatial data. For this I use GeoLocator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the city names are corrected, the city names can be used to collect the coordinates from GeoLocator:\n",
    "new = csf['City'].str.split()\n",
    "csf['City'] = new.str[0]\n",
    "\n",
    "csf = csf.replace(to_replace ='Ohmihachiman', value ='Omihachiman')  ## Ohmihachiman does not work with GeoLocator\n",
    "\n",
    "csf['Prefecture'] = np.where(csf['City'] == 'Tahara', 'Aichi', csf['Prefecture']) ## Report 26 states two outbreaks in Tahara, Gifu, but Tahara is in Aichi\n",
    "\n",
    "csf['Geo_Lookup'] = csf['City'] + ' ' + csf['Prefecture']\n",
    "csf = csf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBS: Very slow (6 min.) - to avoid being cut off I include a timeout ###\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"school project\")\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "for city in csf['Geo_Lookup']:\n",
    "    try:\n",
    "        location = geolocator.geocode(city, timeout=50)\n",
    "        lat = location.latitude\n",
    "        lon = location.longitude\n",
    "        latitude.append(lat)\n",
    "        longitude.append(lon)\n",
    "    except:\n",
    "        print(city)      # If an error occurs the whole loop stops. This is included to know what throws the error.\n",
    "        raise\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "# I check that the length matches the number of observations:\n",
    "print(len(latitude))\n",
    "print(len(longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 270 entries, 0 to 269\n",
      "Data columns (total 2 columns):\n",
      "Latitude     270 non-null float64\n",
      "Longitude    270 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 4.3 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 270 entries, 0 to 269\n",
      "Data columns (total 9 columns):\n",
      "ID              270 non-null object\n",
      "City            270 non-null object\n",
      "Prefecture      270 non-null object\n",
      "Date            270 non-null object\n",
      "Species         270 non-null object\n",
      "Susceptibles    270 non-null object\n",
      "Cases           270 non-null object\n",
      "Deaths          270 non-null object\n",
      "Geo_Lookup      270 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 19.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The list with the coordinates are converted to a dataframe\n",
    "coordinates = pd.DataFrame({'Latitude': latitude,\n",
    "                              'Longitude': longitude,                            \n",
    "})\n",
    "\n",
    "# I check the compatability\n",
    "print(coordinates.info())\n",
    "print(csf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID  City Prefecture        Date                       Species  \\\n",
      "0  on.1  Gifu       Gifu  2018-09-03                         Swine   \n",
      "1   1.1  Gifu       Gifu  2018-09-13  Wild boar:Sus scrofa(Suidae)   \n",
      "2   2.1  Gifu       Gifu  2018-09-15  Wild boar:Sus scrofa(Suidae)   \n",
      "3   2.2  Gifu       Gifu  2018-09-18  Wild boar:Sus scrofa(Suidae)   \n",
      "\n",
      "  Susceptibles Cases Deaths Geo_Lookup   Latitude   Longitude  \n",
      "0          610    29     29  Gifu Gifu  35.423095  136.762753  \n",
      "1            0     1      1  Gifu Gifu  35.423095  136.762753  \n",
      "2            0     1      1  Gifu Gifu  35.423095  136.762753  \n",
      "3            0     3      3  Gifu Gifu  35.423095  136.762753  \n"
     ]
    }
   ],
   "source": [
    "# The coordinates-dataframe is appended to the big dataset \n",
    "csf['Latitude'] = coordinates['Latitude']\n",
    "csf['Longitude'] = coordinates['Longitude']\n",
    "\n",
    "csf = csf\n",
    "\n",
    "print(csf.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = csf.to_csv('CSF_Japan_data.csv', index = False)\n",
    "\n",
    "# I create a folder for all generated datasets, just for safe keeping.\n",
    "if not os.path.exists('Old datasets'):\n",
    "    os.mkdir('Old datasets')\n",
    "\n",
    "# I save an additional CSV with todays date to the old datasets folder.\n",
    "todays_date = datetime.now().strftime('%d%m%Y')\n",
    "export_csv_date = csf.to_csv('Old datasets/CSF_Outbreaks_Japan_'+todays_date+'.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
